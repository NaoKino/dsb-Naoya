---
title: "Homework 3"
author: "Naoya Kinoshita"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

------------------------------------------------------------------------

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(tictoc)
library(skimr)
library(countrycode)
library(here)
library(DBI)
library(dbplyr)
library(arrow)
library(rvest)
library(robotstxt) # check if we're allowed to scrape the data
library(scales)
library(sf)
library(readxl)
library(cowplot)
library(gridExtra)
library(purrr)
```

# Money in UK politics

[The Westminster Accounts](https://news.sky.com/story/the-westminster-accounts-12786091), a recent collaboration between Sky News and Tortoise Media, examines the flow of money through UK politics. It does so by combining data from three key sources:

1.  [Register of Members' Financial Interests](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-members-financial-interests/),
2.  [Electoral Commission records of donations to parties](http://search.electoralcommission.org.uk/English/Search/Donations), and
3.  [Register of All-Party Parliamentary Groups](https://www.parliament.uk/mps-lords-and-offices/standards-and-financial-interests/parliamentary-commissioner-for-standards/registers-of-interests/register-of-all-party-party-parliamentary-groups/).

You can [search and explore the results](https://news.sky.com/story/westminster-accounts-search-for-your-mp-or-enter-your-full-postcode-12771627) through the collaboration's interactive database. Simon Willison [has extracted a database](https://til.simonwillison.net/shot-scraper/scraping-flourish) and this is what we will be working with. If you want to read more about [the project's methodology](https://www.tortoisemedia.com/2023/01/08/the-westminster-accounts-methodology/).

## Open a connection to the database

The database made available by Simon Willison is an `SQLite` database

```{r}
sky_westminster <- DBI::dbConnect(
  drv = RSQLite::SQLite(),
  dbname = here::here("data", "sky-westminster-files.db")
)
```

How many tables does the database have?

```{r}
DBI::dbListTables(sky_westminster)
```

```{r}
glimpse(tbl(sky_westminster, "appg_donations"))
glimpse(tbl(sky_westminster, "appgs"))
```

```{r}

# Checking data of appg_donations
checking_data_appg_donations <- tbl(sky_westminster, "appg_donations") %>%
  group_by(entity) %>%
  summarise(checking_data_appg_donations = sum(value))

# Valueが大きい順に並び替え
top_20_entities <- checking_data_appg_donations %>%
  arrange(desc(checking_data_appg_donations)) %>%
  head(20)

# 棒グラフで上位20のentityを表示
ggplot(top_20_entities, aes(x = reorder(entity, checking_data_appg_donations), y = checking_data_appg_donations)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  xlab("Entity") +
  ylab("Amount of Donations") +
  ggtitle("Top 20 Entities by Amount of Donations") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

```{r}
glimpse(tbl(sky_westminster, "member_appgs"))
glimpse(tbl(sky_westminster, "members"))
```

```{r}
glimpse(tbl(sky_westminster, "parties"))
glimpse(tbl(sky_westminster, "party_donations"))
glimpse(tbl(sky_westminster, "payments"))
```

## Which MP has received the most amount of money?

You need to work with the `payments` and `members` tables and for now we just want the total among all years. To insert a new, blank chunk of code where you can write your beautiful code (and comments!), please use the following shortcut: `Ctrl + Alt + I` (Windows) or `cmd + option + I` (mac)

#### Answer: Theresa May has received the most amount of money, which is 2809765.4.

#### I'm not sure but following code does work when conducting "Run current chunk", but does not work when I Knit it, so just for the temporaly measure, I will reject this code.

# Loop through each table and print its contents
for (table in tables) {
  query <- paste("SELECT * FROM", table)
  result <- DBI::dbGetQuery(sky_westminster, query)
  
  cat("Table:", table, "\n")
  if (nrow(result) > 0) {
    print(result)
  } else {
    cat("No data available\n")
  }
  cat("\n")
}


```{r}
# Extract the top 10 rows from the aggregated payments data with member details using dplyr
query_2 <- "
  SELECT member_id, SUM(value) as total_value
  FROM payments
  GROUP BY member_id
  ORDER BY total_value DESC
  LIMIT 10
"
results_4 <- DBI::dbGetQuery(sky_westminster, query_2)

# Check if the columns exist in the members table
if (all(c("id", "name") %in% colnames(tbl(sky_westminster, "members")))) {
  members <- tbl(sky_westminster, "members") %>%
    collect()  # Add this line
  results_4 <- results_4 %>%
    inner_join(members, by = c("member_id" = "id")) %>%
    select(member_id, total_value, name)
} else {
  stop("The columns 'id' and/or 'name' do not exist in the 'members' table.")
}

# Print the results
print(results_4)
```

## Any `entity` that accounts for more than 5% of all donations?

Is there any `entity` whose donations account for more than 5% of the total payments given to MPs over the 2020-2022 interval? Who are they and who did they give money to?

#### Answer: As shown in below table, Unite and Lord David Sainsbury are the entity whose donations account for more than 5% of the total payments given to MPs.

```{r}

# Calculate the total payments over the 2020-2022 interval

total_payments <- tbl(sky_westminster, "payments") %>%
  mutate(year=str_sub(date,-4)) %>% 
  group_by(year, id) %>%
  summarise(total_payments = sum(value)) %>%
  filter(year >= "2020" & year <= "2022") %>%
  collect()

# Find entities whose donations account for more than 5% of the total payments
  results_morethan5 <- tbl(sky_westminster, "party_donations") %>%
  mutate(year = str_sub(date, 4)) %>%
  group_by(entity) %>%
  summarise(total_donations = sum(value)) %>%
  mutate(percentage = total_donations / sum(total_donations) * 100) %>%
  slice_max(order_by = total_donations, n = 10) %>%
  collect()

# Print the results
print(results_morethan5)

```

## Do `entity` donors give to a single party or not?

-   How many distinct entities who paid money to MPS(Members of Parliament) are there? Answer: There are 1,077 entities who paid money to MPS.

-   How many (as a number and %) donated to MPs belonging to a single party only?

```{r}
# How many distinct entities who paid money to MPS are there?

distinct_entities <- tbl(sky_westminster, "payments") %>%
  filter(category == "2. (a) Support linked to an MP but received by a local party organisation or indirectly via a central party organisation") %>%
  distinct(entity) %>%
  count(entity)

# Print the result
print(distinct_entities)

# Count the entities who donated to MPs belonging to a single party only
single_party_donors <- tbl(sky_westminster, "party_donations") %>%
  group_by(entity) %>%
  summarise(distinct_parties = n_distinct(entity)) %>%
  summarise(
    single_party_donors = sum(distinct_parties == 1),
    percentage = sum(distinct_parties == 1) / n() * 100) %>%
  collect()

# Print the results
print(distinct_entities)
print(single_party_donors)

```

## Which party has raised the greatest amount of money in each of the years 2020-2022?

I would like you to write code that generates the following table. Answer: Please find the code shown as below.

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics(here::here("images", "total_donations_table.png"), error = FALSE)

# Calculate total donations by year and entity, joining with parties table
greatest_amount <- tbl(sky_westminster, "party_donations") %>%
  mutate(year = as.integer(str_sub(date, 1, 4))) %>%
  filter(year >= 2020 & year <= 2022) %>%
  group_by(year, party_id) %>%
  summarise(total_donations = sum(value)) %>%
  left_join(tbl(sky_westminster, "parties"), by = c("party_id" = "id")) %>%
  select(year, party_name = name, total_donations) %>%
  ungroup()

print(greatest_amount)

# Create separate data frames for each year
df_2020 <- filter(greatest_amount, year == 2020)
df_2021 <- filter(greatest_amount, year == 2021)
df_2022 <- filter(greatest_amount, year == 2022)

# Create bar plots for each year
plot_2020 <- ggplot(df_2020, aes(x = party_name, y = total_donations)) +
  geom_bar(stat = "identity", fill = "#E69F00") +
  xlab("Party Name") +
  ylab("Total Donations") +
  ggtitle("Total Donations by Party (2020)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

plot_2021 <- ggplot(df_2021, aes(x = party_name, y = total_donations)) +
  geom_bar(stat = "identity", fill = "#56B4E9") +
  xlab("Party Name") +
  ylab("Total Donations") +
  ggtitle("Total Donations by Party (2021)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

plot_2022 <- ggplot(df_2022, aes(x = party_name, y = total_donations)) +
  geom_bar(stat = "identity", fill = "#009E73") +
  xlab("Party Name") +
  ylab("Total Donations") +
  ggtitle("Total Donations by Party (2022)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Display the plots
plot_2020
plot_2021
plot_2022  

```

... and then, based on this data, plot the following graph.

```{r echo=FALSE, out.width="80%"}
knitr::include_graphics(here::here("images", "total_donations_graph.png"), error = FALSE)
# Calculate total donations by year and entity, joining with parties table
greatest_amount <- tbl(sky_westminster, "party_donations") %>%
  mutate(year = as.integer(str_sub(date, 1, 4))) %>%
  filter(year %in% c(2020, 2021, 2022)) %>%
  group_by(year, party_id) %>%
  summarise(total_donations = sum(value)) %>%
  left_join(tbl(sky_westminster, "parties"), by = c("party_id" = "id")) %>%
  arrange(year, desc(total_donations))

# Create the bar plot
ggplot(greatest_amount, aes(x = year, y = total_donations, fill = name)) +
  geom_bar(stat = "identity", position = "dodge") +
  xlab("Year") +
  ylab("Total Donations") +
  ggtitle("Total Donations by Year and Party") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  scale_fill_manual(values = c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7", "#999999", "#EFEFEF", "#FF8D34"))
```

This uses the default ggplot colour pallete, as I dont want you to worry about using the [official colours for each party](https://en.wikipedia.org/wiki/Wikipedia:Index_of_United_Kingdom_political_parties_meta_attributes). However, I would like you to ensure the parties are sorted according to total donations and not alphabetically. You may even want to remove some of the smaller parties that hardly register on the graph. Would facetting help you?

Finally, when you are done working with the databse, make sure you close the connection, or disconnect from the database.

```{r}
dbDisconnect(sky_westminster)
```

# Anonymised Covid patient data from the CDC

We will be using a dataset with [anonymous Covid-19 patient data that the CDC publishes every month](https://data.cdc.gov/Case-Surveillance/COVID-19-Case-Surveillance-Public-Use-Data-with-Ge/n8mc-b4w4). The file we will use was released on April 11, 2023, and has data on 98 million of patients, with 19 features. This file cannot be loaded in memory, but luckily we have the data in `parquet` format and we will use the `{arrow}` package.

## Obtain the data

The dataset `cdc-covid-geography` in in `parquet` format that {arrow}can handle. It is \> 600Mb and too large to be hosted on Canvas or Github, so please download it from dropbox <https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0> and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false


tic() # start timer
cdc_data <- open_dataset(here::here("data", "cdc-covid-geography"))
toc() # stop timer


glimpse(cdc_data)
```

### Understanding data

```{r}
# Absolute Number
# 1) Filter data for Male sex only
male_data <- cdc_data %>%
  filter(sex == "Male", icu_yn =="Yes", current_status == "Laboratory-confirmed case")

# 2) Aggregate data by death_yn and age_group
male_age_death_counts <- male_data %>%
  group_by(death_yn, age_group) %>%
  summarise(count = n()) %>%
  ungroup()

# Convert data frame to regular data frame
male_age_death_counts <- as.data.frame(male_age_death_counts)

# 3) Create a plot with age_group on the x-axis and count on the y-axis, colored by death_yn
plot_male <- ggplot(male_age_death_counts, aes(x = age_group, y = count, fill = death_yn)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Age Group", y = "Count", title = "Number of Cases by Age Group and Death Status (Male)") +
  theme_bw()

# Display the plot
plot_male
```

```{r}
# 1) Filter data for Male sex only
male_data_icu <- cdc_data %>%
  filter(sex == "Male", current_status == "Laboratory-confirmed case")

# 2) Aggregate data by death_icu and age_group
male_age_icu_counts <- male_data_icu %>%
  group_by(icu_yn, age_group) %>%
  summarise(count = n()) %>%
  ungroup()

# Convert data frame to regular data frame
male_age_icu_counts <- as.data.frame(male_age_icu_counts)

# 3) Create a plot with age_group on the x-axis and count on the y-axis, colored by death_yn
plot_male_icu <- ggplot(male_age_icu_counts, aes(x = age_group, y = count, fill = icu_yn)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Age Group", y = "Count", title = "Number of Cases by Age Group and ICU Status (Male)") +
  theme_bw()

# Display the plot
plot_male_icu
```

Can you query the database and replicate the following plot?

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "covid-CFR-ICU.png"), error = FALSE)
```

```{r}
# 1) Filter data for Male sex only and specific conditions
male_data_yes <- cdc_data %>%
  filter(sex == "Male", icu_yn == "Yes", current_status == "Laboratory-confirmed case")

# 2) Aggregate data by death_yn and age_group
male_age_death_counts <- male_data_yes %>%
  group_by(age_group) %>%
  summarise(death_ratio = sum(death_yn == "Yes", na.rm = TRUE) / n())

# Convert data frame to regular data frame
male_age_death_counts <- as.data.frame(male_age_death_counts)

# 3) Create a plot with age_group on the x-axis and death_ratio on the y-axis
plot_male_following_plot_icu_yes <- ggplot(male_age_death_counts, aes(x = age_group, y = death_ratio, fill = age_group)) +
  geom_col() +
  labs(x = "Age Group", y = "Death Ratio", title = "CFR % by Age Group (Male)") +
  theme_bw()

# Display the plot
plot_male_following_plot_icu_yes
```

```{r}
# 1) Filter data for Male sex only and specific conditions
feamale_data_yes <- cdc_data %>%
  filter(sex == "Female", icu_yn == "Yes", current_status == "Laboratory-confirmed case")

# 2) Aggregate data by death_yn and age_group
feamale_age_death_counts_yes <- feamale_data_yes %>%
  group_by(age_group) %>%
  summarise(death_ratio = sum(death_yn == "Yes", na.rm = TRUE) / n())

# Convert data frame to regular data frame
feamale_age_death_counts_yes <- as.data.frame(feamale_age_death_counts_yes)

# 3) Create a plot with age_group on the x-axis and death_ratio on the y-axis
plot_female_following_plot_icu_yes <- ggplot(feamale_age_death_counts_yes, aes(x = age_group, y = death_ratio, fill = age_group)) +
  geom_col() +
  labs(x = "Age Group", y = "Death Ratio", title = "CFR % by Age Group (Female)") +
  theme_bw()

# Display the plot
plot_female_following_plot_icu_yes
```

```{r}
# 1) Filter data for Male sex only and specific conditions
male_data_no <- cdc_data %>%
  filter(sex == "Male", icu_yn == "No", current_status == "Laboratory-confirmed case")

# 2) Aggregate data by death_yn and age_group
male_age_death_counts_no <- male_data_no %>%
  group_by(age_group) %>%
  summarise(death_ratio = sum(death_yn == "Yes", na.rm = TRUE) / n())

# Convert data frame to regular data frame
male_age_death_counts_no <- as.data.frame(male_age_death_counts_no)

# 3) Create a plot with age_group on the x-axis and death_ratio on the y-axis
plot_male_following_plot_icu_no <- ggplot(male_age_death_counts_no, aes(x = age_group, y = death_ratio, fill = age_group)) +
  geom_col() +
  labs(x = "Age Group", y = "Death Ratio", title = "CFR % by Age Group (Male/Non ICU)") +
  theme_bw()

# Display the plot
plot_male_following_plot_icu_no
```

```{r}
# 1) Filter data for Male sex only and specific conditions
female_data_no <- cdc_data %>%
  filter(sex == "Female", icu_yn == "No", current_status == "Laboratory-confirmed case")

# 2) Aggregate data by death_yn and age_group
female_age_death_counts_no <- female_data_no %>%
  group_by(age_group) %>%
  summarise(death_ratio = sum(death_yn == "Yes", na.rm = TRUE) / n())

# Convert data frame to regular data frame
female_age_death_counts_no <- as.data.frame(female_age_death_counts_no)

# 3) Create a plot with age_group on the x-axis and death_ratio on the y-axis
plot_female_following_plot_icu_no <- ggplot(female_age_death_counts_no, aes(x = age_group, y = death_ratio, fill = age_group)) +
  geom_col() +
  labs(x = "Age Group", y = "Death Ratio", title = "CFR % by Age Group (Female/Non ICU)") +
  theme_bw()

# Display the plot
plot_female_following_plot_icu_no
```

```{r}
# Combine the plots
combined_plot <- grid.arrange(plot_male_following_plot_icu_yes, plot_male_following_plot_icu_no, plot_female_following_plot_icu_yes, plot_female_following_plot_icu_no, ncol = 2)

# Display the combined plot
print(combined_plot)
```

The previous plot is an aggregate plot for all three years of data. What if we wanted to plot Case Fatality Ratio (CFR) over time? Write code that collects the relevant data from the database and plots the following

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "cfr-icu-overtime.png"), error = FALSE)

# Filter data for Male sex only and specific conditions
male_data <- cdc_data %>%
  filter(sex == "Male", icu_yn == "Yes", current_status == "Laboratory-confirmed case")

# Aggregate data by case_month, age_group, and count the number of cases and deaths
male_monthly_counts <- male_data %>%
  group_by(age_group, case_month) %>%
  summarise(total_cases = n(), total_deaths = sum(death_yn == "Yes", na.rm = TRUE)) %>%
  ungroup()

# Calculate the Case Fatality Ratio (CFR) as deaths per total cases
male_monthly_counts <- male_monthly_counts %>%
  mutate(CFR = (total_deaths / total_cases) * 100)

# Create a line plot of CFR over time for each age group
plot_cfr <- ggplot(as.data.frame(male_monthly_counts), aes(x = case_month, y = CFR, color = age_group)) +
  geom_line() +
  geom_point() +
  labs(x = "Month", y = "Case Fatality Ratio (%)", title = "Case Fatality Ratio (CFR) over Time (Male)") +
  theme_bw()

# Display the plot
plot_cfr
```

```{r}
# Define a function to filter the data and calculate the CFR
calculate_cfr <- function(data, sex, icu_status) {
  filtered_data <- data %>%
    filter(sex == sex, icu_yn == icu_status, current_status == "Laboratory-confirmed case") %>%
    group_by(age_group, case_month) %>%
    summarise(total_cases = n(), total_deaths = sum(death_yn == "Yes", na.rm = TRUE)) %>%
    ungroup() %>%
    mutate(CFR = (total_deaths / total_cases) * 100)
  
  return(as.data.frame(filtered_data))
}

# Apply the function to each condition
male_icu_yes <- calculate_cfr(cdc_data, "Male", "Yes")
male_icu_no <- calculate_cfr(cdc_data, "Male", "No")
female_icu_yes <- calculate_cfr(cdc_data, "Female", "Yes")
female_icu_no <- calculate_cfr(cdc_data, "Female", "No")

# Define a function to create a plot
create_plot <- function(data, title) {
  plot <- ggplot(data, aes(x = case_month, y = CFR, color = age_group)) +
    geom_line() +
    geom_point() +
    labs(x = "Month", y = "Case Fatality Ratio (%)", title = title) +
    theme_bw()
  
  return(plot)
}

# Create a plot for each condition
plot_male_icu_yes <- create_plot(male_icu_yes, "Case Fatality Ratio (CFR) over Time (Male, ICU)")
plot_male_icu_no <- create_plot(male_icu_no, "Case Fatality Ratio (CFR) over Time (Male, Non-ICU)")
plot_female_icu_yes <- create_plot(female_icu_yes, "Case Fatality Ratio (CFR) over Time (Female, ICU)")
plot_female_icu_no <- create_plot(female_icu_no, "Case Fatality Ratio (CFR) over Time (Female, Non-ICU)")

# Combine the plots into a 2x2 grid
combined_plot_2 <- grid.arrange(plot_male_icu_yes, plot_male_icu_no, plot_female_icu_yes, plot_female_icu_no, ncol = 2)

# Display the combined plot
print(combined_plot_2)
```

For each patient, the dataframe also lists the patient's states and county [FIPS code](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code). The CDC also has information on the [NCHS Urban-Rural classification scheme for counties](https://www.cdc.gov/nchs/data_access/urban_rural.htm)

```{r}
urban_rural <- read_xlsx(here::here("data", "NCHSURCodes2013.xlsx")) %>% 
  janitor::clean_names() 
```

### Understanding Data

```{r}
glimpse(urban_rural)

#country_data <- urban_rural %>%
 # group_by(x2013_code)
  #left_join((cdc_data), by = c("country_code" = "flips_code")) %>%
```

Each county belongs in seix diffent categoreis, with categories 1-4 being urban areas and categories 5-6 being rural, according to the following criteria captured in `x2013_code`

Category name

1.  Large central metro - 1 million or more population and contains the entire population of the largest principal city
2.  large fringe metro - 1 million or more poulation, but does not qualify as 1
3.  Medium metro - 250K - 1 million population
4.  Small metropolitan population \< 250K
5.  Micropolitan
6.  Noncore

Can you query the database, extract the relevant information, and reproduce the following two graphs that look at the Case Fatality ratio (CFR) in different counties, according to their population?

Answer: The data with two graphs that look at the Case Fatality ratio (CFR) in different counties. However, my calculation of CFR looks not good, so there is data which plotted not looks like the example.

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "cfr-county-population.png"), error = FALSE)


# Data preprocessing
# Extract necessary information from cdc_data, join with urban_rural
data <- cdc_data %>%
  filter(!is.na(res_county), !is.na(county_fips_code)) %>%
  group_by(res_county, county_fips_code, case_month) %>%
  summarise(
    total_cases = n(),
    total_deaths = sum(death_yn == "Yes"),
    cfr = total_deaths / total_cases * 100
  ) %>%
  collect() %>% # Convert Arrow dplyr query to a data frame
  left_join(urban_rural, by = c("county_fips_code" = "fips_code")) %>%
  filter(!is.na(cfr), !is.na(county_2012_pop), !is.na(x2013_code)) # Remove rows with missing values

# Create a list to store each graph
plots <- list()

# Loop through each category of x2013_code
for(i in 1:6){
  # Filter data for the current category
  data_i <- data %>%
    filter(x2013_code == i)
  
  # Create a graph for the current category
  p <- ggplot(data_i, aes(x = case_month, y = cfr)) +
    geom_line() +
    labs(x = "Date", y = "CFR (%)", title = paste("CFR by Date for x2013_code =", i))
  
  # Add the graph to the list
  plots[[i]] <- p
}

# Combine all graphs into a single plot
combined_plot <- gridExtra::grid.arrange(grobs = plots, ncol = 2)

# Save the combined plot to a JPEG file
ggsave("combined_plot.jpeg", combined_plot)
```

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "cfr-rural-urban.png"), error = FALSE)

# Data preprocessing
# Extract necessary information from cdc_data, join with urban_rural
data <- cdc_data %>%
  filter(!is.na(res_county), !is.na(county_fips_code)) %>%
  group_by(res_county, county_fips_code, case_month) %>%
  summarise(
    total_cases = n(),
    total_deaths = sum(death_yn == "Yes"),
    cfr = total_deaths / total_cases * 100
  ) %>%
  collect() %>% # Convert Arrow dplyr query to a data frame
  left_join(urban_rural, by = c("county_fips_code" = "fips_code")) %>%
  filter(!is.na(cfr), !is.na(county_2012_pop), !is.na(x2013_code)) %>% # Remove rows with missing values
  mutate(
    area_type = ifelse(x2013_code %in% 1:3, "Urban", "Rural") # Create a new variable for area type
  )

# Create a line graph showing the time series change of CFR% for both Urban and Rural areas
ggplot(data, aes(x = case_month, y = cfr, color = area_type)) +
  geom_line() +
  labs(x = "Date", y = "CFR (%)", color = "Area Type", title = "CFR by Date for Urban and Rural Areas")

```

# Money in US politics

In the United States, [*"only American citizens (and immigrants with green cards) can contribute to federal politics, but the American divisions of foreign companies can form political action committees (PACs) and collect contributions from their American employees."*](https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs)

We will scrape and work with data foreign connected PACs that donate to US political campaigns. The data for foreign connected PAC contributions in the 2022 election cycle can be found at <https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022>. Then, we will use a similar approach to get data such contributions from previous years so that we can examine trends over time.

All data come from [OpenSecrets.org](https://www.opensecrets.org), a *"website tracking the influence of money on U.S. politics, and how that money affects policy and citizens' lives"*.

```{r}
#| label: allow-scraping-opensecrets
#| warning: false
#| message: false

library(robotstxt)
paths_allowed("https://www.opensecrets.org")

base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"

contributions_tables <- base_url %>%
  read_html() 

```

-   First, make sure you can scrape the data for 2022. Use janitor::clean_names() to rename variables scraped using `snake_case` naming.

-   Clean the data:

    -   Write a function that converts contribution amounts in `total`, `dems`, and `repubs` from character strings to numeric values.
    -   Separate the `country_of_origin_parent_company` into two such that country and parent company appear in different columns for country-level analysis.

```{r}
# Define a function to parse currency values
parse_currency <- function(x){
  x %>%
    # Remove dollar signs
    str_remove("\\$") %>%
    # Remove all occurrences of commas
    str_remove_all(",") %>%
    # Convert to numeric
    as.numeric()
}

# Check if scraping is allowed on the website
paths_allowed("https://www.opensecrets.org")

# Define the base URL
base_url <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"

# Read the HTML of the webpage
webpage <- base_url %>%
  read_html() 

# Extract the table from the webpage
contributions <- webpage %>%
  html_nodes("table") %>% # Identify the table in the webpage
  .[[1]] %>% # Select the first table
  html_table(fill = TRUE) # Convert the table to a data frame

# Print column names
glimpse(contributions)

# Clean the country/parent company and contributions data
contributions_changed <- contributions %>%
  # Separate the 'country_of_origin_parent_company' column into 'country' and 'parent' columns
  separate("Country of Origin/Parent Company", 
           into = c("country", "parent"), 
           sep = "\\s*/\\s*", 
           extra = "merge") %>%
  # Apply the 'parse_currency' function to the 'total', 'dems', and 'repubs' columns
  mutate(
    "Total" = parse_currency(Total),
    "Dems" = parse_currency(Dems),
    "Repubs" = parse_currency(Repubs)
  )

glimpse(contributions_changed)

```

-   Write a function called `scrape_pac()` that scrapes information from the Open Secrets webpage for foreign-connected PAC contributions in a given year. This function should

    -   have one input: the URL of the webpage and should return a data frame.
    -   add a new column to the data frame for `year`. We will want this information when we ultimately have data from all years, so this is a good time to keep track of it. Our function doesn't take a year argument, but the year is embedded in the URL, so we can extract it out of there, and add it as a new column. Use the `str_sub()` function to extract the last 4 characters from the URL. You will probably want to look at the help for this function to figure out how to specify "last 4 characters".

-   Define the URLs for 2022, 2020, and 2000 contributions. Then, test your function using these URLs as inputs. Does the function seem to do what you expected it to do?

-   Construct a vector called `urls` that contains the URLs for each webpage that contains information on foreign-connected PAC contributions for a given year.

-   Map the `scrape_pac()` function over `urls` in a way that will result in a data frame called `contributions_all`.

-   Write the data frame to a csv file called `contributions-all.csv` in the `data` folder. 

## Answer: Please find the code which satisfy instruction. However, some issues with writting the data frame to csv file, so I muted last part of code which is shown as below. #write.csv(contributions_all, file = "data/contributions-all.csv")

```{r}
# Define the function
scrape_pac <- function(url) {
  # Use tryCatch to handle potential errors
  result <- tryCatch({
    # Read the HTML of the webpage
    webpage <- read_html(url)
    
    # Extract the table from the webpage
    table <- webpage %>%
      html_nodes("table") %>% # Identify the table in the webpage
      .[[1]] %>% # Select the first table
      html_table(fill = TRUE) # Convert the table to a data frame
    
    # Clean the data
    cleaned_table <- table %>%
      # Remove rows with missing PAC names
      filter(!is.na(`PAC Name (Affiliate)`)) %>%
      # Separate the 'Country of Origin/Parent Company' column
      separate("Country of Origin/Parent Company", into = c("Country", "Parent Company"), sep = "\\s*/\\s*", extra = "merge") %>%
      # Parse the currency columns
      mutate(
        "Total" = parse_currency(Total),
        "Dems" = parse_currency(Dems),
        "Repubs" = parse_currency(Repubs)
      ) %>%
      # Add the year column
      mutate(Year = as.integer(str_sub(url, -4)))
    
    return(cleaned_table)
  }, error = function(e) {
    # If there's an error, return NULL
    return(NULL)
  })
  
  return(result)
}

# Define the URLs
url_2022 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2022"
url_2020 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2020"
url_2000 <- "https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/2000"

# Test the function
contributions_2022 <- scrape_pac(url_2022)
contributions_2020 <- scrape_pac(url_2020)
contributions_2000 <- scrape_pac(url_2000)

# Construct the URLs vector
urls <- paste0("https://www.opensecrets.org/political-action-committees-pacs/foreign-connected-pacs/", 2000:2022)

# Map the function over the URLs
contributions_all <- map_dfr(urls, scrape_pac)

# Remove NULL results (i.e., years with no data)
contributions_all <- contributions_all %>% filter(!is.null(Year))

# Write the data frame to a csv file
#write.csv(contributions_all, file = "data/contributions-all.csv")

```

# Scraping consulting jobs

The website [https://www.consultancy.uk/jobs/](https://www.consultancy.uk/jobs) lists job openings for consulting jobs.

```{r}
#| label: consulting_jobs_url
#| eval: false

library(robotstxt)
paths_allowed("https://www.consultancy.uk") #is it ok to scrape?

base_url <- "https://www.consultancy.uk/jobs/page/1"

listings_html <- base_url %>%
  read_html()

```

Identify the CSS selectors in order to extract the relevant information from this page, namely

1.  job
2.  firm
3.  functional area
4.  type

Can you get all pages of ads, and not just the first one, `https://www.consultancy.uk/jobs/page/1` into a dataframe?

-   Write a function called `scrape_jobs()` that scrapes information from the webpage for consulting positions. This function should

    -   have one input: the URL of the webpage and should return a data frame with four columns (variables): job, firm, functional area, and type

    -   Test your function works with other pages too, e.g., <https://www.consultancy.uk/jobs/page/2>. Does the function seem to do what you expected it to do?

    -   Given that you have to scrape `...jobs/page/1`, `...jobs/page/2`, etc., define your URL so you can join multiple stings into one string, using `str_c()`. For instnace, if `page` is 5, what do you expect the following code to produce?

```         
base_url <- "https://www.consultancy.uk/jobs/page/1"
url <- str_c(base_url, page)
```

-   Construct a vector called `pages` that contains the numbers for each page available

-   Map the `scrape_jobs()` function over `pages` in a way that will result in a data frame called `all_consulting_jobs`.

-   Write the data frame to a csv file called `all_consulting_jobs.csv` in the `data` folder. 

## Answer: Please find the code which satisfy instruction. However, some issues with writting the data frame to csv file, so I muted last part of code which is shown as below. #write.csv(all_consulting_jobs, file = "data/all_consulting_jobs.csv", row.names = FALSE)

```{r}
# Define a function to scrape job listings
scrape_jobs <- function(url) {
  # Read the HTML of the webpage
  webpage <- url %>%
    read_html()
  
  # Extract the job listings from the webpage
  job_listings <- webpage %>%
    html_nodes(".job-listing") %>%
    map_df(~{
      data.frame(
        job = .x %>% html_node(".job-title") %>% html_text(trim = TRUE),
        firm = .x %>% html_node(".job-company") %>% html_text(trim = TRUE),
        functional_area = .x %>% html_node(".job-functional-area") %>% html_text(trim = TRUE),
        type = .x %>% html_node(".job-type") %>% html_text(trim = TRUE),
        stringsAsFactors = FALSE
      )
    })
  
  return(job_listings)
}

# Define the base URL and the number of pages to scrape
base_url <- "https://www.consultancy.uk/jobs/page/"
pages <- 1:8

# Construct a vector of URLs
urls <- str_c(base_url, pages)

# Map the 'scrape_jobs()' function over 'urls'
all_consulting_jobs <- map_df(urls, scrape_jobs)

# Write the data frame to a csv file
#write.csv(all_consulting_jobs, file = "data/all_consulting_jobs.csv", row.names = FALSE)

```

# Create a shiny app - OPTIONAL

We have already worked with the data on electricity production and usage, GDP/capita and CO2/capita since 1990. You have to create a simple Shiny app, where a user chooses a country from a drop down list and a time interval between 1990 and 2020 and shiny outputs the following

```{r echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images", "electricity-shiny.png"), error = FALSE)
```

You can use chatGPT to get the basic layout of Shiny app, but you need to adjust the code it gives you. Ask chatGPT to create the Shiny app using the `gapminder` data and make up similar requests for the inputs/outpus you are thinking of deploying.

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (Rmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be commiting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: ChatGPT
-   Approximately how much time did you spend on this problem set: 10-12hr
-   What, if anything, gave you the most trouble: Understanding the database and connecting the table to each other was the hardest thing. In addition, web scraping is quite tricky, which needs more training.

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
